{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85a6d4b-c7aa-47b1-9fc7-4b63727f6829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted lines have been saved to extracted_target.m2\n"
     ]
    }
   ],
   "source": [
    "def extract_target_lines(input_file, output_file, target_sentences_file):\n",
    "    # Read target sentences from the file\n",
    "    with open(target_sentences_file, 'r', encoding='utf-8') as f:\n",
    "        target_sentences = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    # Open the input m2 file in read mode\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    # Create a list to store the extracted lines\n",
    "    extracted_lines = []\n",
    "\n",
    "    # Iterate through all the target sentences\n",
    "    for target_sentence in target_sentences:\n",
    "        # Iterate through all lines in the m2 file\n",
    "        inside_target_block = False\n",
    "        for line in lines:\n",
    "            # Remove leading \"S \" from the line and check for a match\n",
    "            sentence = line.strip()\n",
    "            if sentence.startswith(\"S \"):\n",
    "                sentence = sentence[2:].strip()\n",
    "\n",
    "            # If the line is the target sentence, we start collecting the block\n",
    "            if sentence == target_sentence:\n",
    "                inside_target_block = True\n",
    "                extracted_lines.append(line)  # Add the sentence itself\n",
    "            # If we are inside a block, continue collecting\n",
    "            elif inside_target_block:\n",
    "                if not line.startswith(\"S \"):\n",
    "                    extracted_lines.append(line)\n",
    "                # If we reach the next sentence block (starts with \"S\"), stop\n",
    "                if line.startswith(\"S \"):\n",
    "                    inside_target_block = False\n",
    "                    \n",
    "\n",
    "    # Write the extracted lines to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.writelines(extracted_lines)\n",
    "\n",
    "# Usage\n",
    "input_file = 'official-2014.combined-withalt.m2'  # input m2 file\n",
    "output_file = 'extracted_target.m2'  # output file for extracted content\n",
    "target_sentences_file = 'INPUT.txt'  # file containing the target sentences\n",
    "\n",
    "# Call the function to extract and save the content\n",
    "extract_target_lines(input_file, output_file, target_sentences_file)\n",
    "\n",
    "print(f\"Extracted lines have been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebce122f-82d6-4143-9de9-f1ce45520d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted lines have been saved to extracted_target_new.m2\n"
     ]
    }
   ],
   "source": [
    "def extract_target_lines(input_file, output_file, target_sentences_file):\n",
    "    # Read target sentences from the file\n",
    "    with open(target_sentences_file, 'r', encoding='utf-8') as f:\n",
    "        target_sentences = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    # Open the input m2 file in read mode\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    # Create a list to store the extracted lines\n",
    "    extracted_lines = []\n",
    "\n",
    "    # Iterate through all the target sentences\n",
    "    for target_sentence in target_sentences:\n",
    "        # Iterate through all lines in the m2 file\n",
    "        inside_target_block = False\n",
    "        for line in lines:\n",
    "            # Remove leading \"S \" from the line and check for a match\n",
    "            sentence = line.strip()\n",
    "            if sentence.startswith(\"S \"):\n",
    "                sentence = sentence[2:].strip()\n",
    "\n",
    "            # If the line is the target sentence, we start collecting the block\n",
    "            if sentence == target_sentence:\n",
    "                inside_target_block = True\n",
    "                extracted_lines.append(line)  # Add the sentence itself\n",
    "            # If we are inside a block, continue collecting\n",
    "            elif inside_target_block:\n",
    "                if not line.startswith(\"S \"):\n",
    "                    extracted_lines.append(line)\n",
    "                # If we reach the next sentence block (starts with \"S\"), stop\n",
    "                if line.startswith(\"S \"):\n",
    "                    inside_target_block = False\n",
    "                    \n",
    "\n",
    "    # Write the extracted lines to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.writelines(extracted_lines)\n",
    "\n",
    "# Usage\n",
    "input_file = 'conll14.errant.m2'  # input m2 file\n",
    "output_file = 'extracted_target_new.m2'  # output file for extracted content\n",
    "target_sentences_file = 'INPUT.txt'  # file containing the target sentences\n",
    "\n",
    "# Call the function to extract and save the content\n",
    "extract_target_lines(input_file, output_file, target_sentences_file)\n",
    "\n",
    "print(f\"Extracted lines have been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752d76a5-6117-4b78-95ec-04f682840db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation complete. M2 file saved to Outputs(SEEDA)/subset/BART.m2\n"
     ]
    }
   ],
   "source": [
    "# Annotation with ERRANT for original sentences (INPUT) and hypothesis sentences (subset output)\n",
    "\n",
    "import errant\n",
    "\n",
    "# Load the Errant annotator\n",
    "annotator = errant.load('en')\n",
    "\n",
    "# Input file paths\n",
    "original_file_path = \"INPUT.txt\"\n",
    "corrected_file_path = \"Outputs(SEEDA)/subset/BART.txt\"\n",
    "\n",
    "# Output file path\n",
    "output_file_path = \"Outputs(SEEDA)/subset/BART.m2\"\n",
    "\n",
    "# Read original and corrected sentences from files\n",
    "with open(original_file_path, 'r', encoding='utf-8') as orig_file:\n",
    "    original_sentences = [line.strip() for line in orig_file.readlines()]\n",
    "\n",
    "with open(corrected_file_path, 'r', encoding='utf-8') as cor_file:\n",
    "    corrected_sentences = [line.strip() for line in cor_file.readlines()]\n",
    "\n",
    "# Open the output file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    # Process each sentence pair\n",
    "    for orig_sentence, cor_sentence in zip(original_sentences, corrected_sentences):\n",
    "        # Parse the sentences\n",
    "        orig = annotator.parse(orig_sentence)\n",
    "        cor = annotator.parse(cor_sentence)\n",
    "\n",
    "        # Annotate the edits\n",
    "        edits = annotator.annotate(orig, cor)\n",
    "\n",
    "        # Write original sentence to the M2 file\n",
    "        output_file.write(f\"S {orig_sentence}\\n\")\n",
    "\n",
    "        # Write each edit to the M2 file\n",
    "        for e in edits:\n",
    "            edit_data = [e.o_start, e.o_end, e.c_start, e.c_end, e.type]\n",
    "            edit = annotator.import_edit(orig, cor, edit_data)\n",
    "            output_file.write(edit.to_m2())\n",
    "            output_file.write(\"\\n\")\n",
    "\n",
    "        # Separate sentence pairs with a newline\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "print(f\"Annotation complete. M2 file saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79775e7c-1c88-4407-b981-93eaf8ef5d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2941\n",
      "0.1515\n",
      "0.7143\n",
      "0.4167\n",
      "0.2941\n",
      "0.5556\n",
      "0.0\n",
      "0.8333\n",
      "1\n",
      "0.625\n",
      "1.0\n",
      "0.8333\n",
      "1\n",
      "0.625\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1\n",
      "1\n",
      "0.625\n",
      "1\n",
      "0.0\n",
      "1.0\n",
      "0.8333\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.5556\n",
      "0.6667\n",
      "0.3571\n",
      "1\n",
      "0.7143\n",
      "0.0\n",
      "1.0\n",
      "1\n",
      "1\n",
      "1.0\n",
      "0.5263\n",
      "0.0\n",
      "0.0\n",
      "0.8333\n",
      "0.5\n",
      "1\n",
      "0.75\n",
      "1.0\n",
      "0.8333\n",
      "0.75\n",
      "0.7143\n",
      "0.625\n",
      "0.7143\n",
      "0.0\n",
      "0.8333\n",
      "0.0\n",
      "0.9091\n",
      "0.0\n",
      "0.0\n",
      "1\n",
      "1.0\n",
      "1\n",
      "0.0\n",
      "0.0\n",
      "1\n",
      "0.6667\n",
      "0.9375\n",
      "1\n",
      "0.0\n",
      "1.0\n",
      "0.7143\n",
      "0.3846\n",
      "0.7143\n",
      "0.625\n",
      "1.0\n",
      "0.7692\n",
      "0.0\n",
      "0.0\n",
      "0.5556\n",
      "0.0\n",
      "1\n",
      "0.7692\n",
      "0.8333\n",
      "1.0\n",
      "0.8333\n",
      "1\n",
      "1\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1\n",
      "1\n",
      "1.0\n",
      "0.6667\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.7143\n",
      "1.0\n",
      "0.7143\n",
      "0.0\n",
      "0.2941\n",
      "0.5\n",
      "0.7143\n",
      "1\n",
      "1.0\n",
      "1.0\n",
      "0.8333\n",
      "1.0\n",
      "0.8333\n",
      "0.5556\n",
      "0.5\n",
      "0.2632\n",
      "0.8333\n",
      "0.5556\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1\n",
      "1.0\n",
      "0.2273\n",
      "0.5556\n",
      "0.0\n",
      "0.8333\n",
      "1.0\n",
      "1.0\n",
      "1\n",
      "0.0\n",
      "1\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.9091\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.625\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.5556\n",
      "0.6\n",
      "1\n",
      "0.7143\n",
      "1\n",
      "0.7692\n",
      "1\n",
      "0.0\n",
      "1.0\n",
      "0.7143\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.3333\n",
      "1\n",
      "0.5\n",
      "0.3333\n",
      "1\n",
      "0.9091\n",
      "0.9091\n",
      "0.8333\n",
      "0.7143\n",
      "0.4167\n",
      "0.0\n",
      "1\n",
      "0.5556\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.8333\n",
      "1\n",
      "0.0\n",
      "1.0\n",
      "1\n",
      "0.3333\n",
      "1\n",
      "0.0\n",
      "0.8333\n",
      "1\n",
      "0.6667\n",
      "0.5263\n",
      "0.625\n",
      "1\n",
      "0.7143\n",
      "0.5\n",
      "0.4167\n",
      "0.0\n",
      "1.0\n",
      "0.5556\n",
      "1\n",
      "0.5882\n",
      "0.0\n",
      "0.8333\n",
      "0.9091\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "0.4545\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5263\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.4545\n",
      "1\n",
      "0.8333\n",
      "1.0\n",
      "1\n",
      "0.7407\n",
      "0.0\n",
      "0.0\n",
      "0.8333\n",
      "0.625\n",
      "0.5\n",
      "0.3846\n",
      "1\n",
      "1\n",
      "0.5556\n",
      "0.9091\n",
      "0.6667\n",
      "0.5263\n",
      "0.0\n",
      "0.7143\n",
      "1.0\n",
      "0.8333\n",
      "1.0\n",
      "0.6667\n",
      "1.0\n",
      "0.3571\n",
      "1.0\n",
      "0.5\n",
      "0.8333\n",
      "1.0\n",
      "0.4762\n",
      "0.4545\n",
      "1\n",
      "0.7143\n",
      "0.625\n",
      "0.4167\n",
      "0.2941\n",
      "0.6667\n",
      "0.8333\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1\n",
      "0.9091\n",
      "1.0\n",
      "0.6667\n",
      "1.0\n",
      "1.0\n",
      "0.3846\n",
      "1.0\n",
      "0.0\n",
      "0.3846\n",
      "1.0\n",
      "0.625\n",
      "0.0\n",
      "0.625\n",
      "0.0\n",
      "1.0\n",
      "0.9091\n",
      "0.7143\n",
      "1.0\n",
      "0.4545\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1\n",
      "0.0\n",
      "0.0\n",
      "1\n",
      "0.8333\n",
      "1.0\n",
      "1.0\n",
      "0.8333\n",
      "0.7143\n",
      "1.0\n",
      "0.7692\n",
      "0.5556\n",
      "0.5\n",
      "0.0\n",
      "0.8333\n",
      "1.0\n",
      "0.3333\n",
      "1.0\n",
      "0.7143\n",
      "1.0\n",
      "0.625\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.2632\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.5357\n",
      "1.0\n",
      "0.625\n",
      "0.8333\n",
      "0.0\n",
      "0.8333\n",
      "0.4878\n",
      "0.6897\n",
      "0.5\n",
      "1.0\n",
      "0.625\n",
      "0.8333\n",
      "1.0\n",
      "0.5\n",
      "0.0\n",
      "0.0\n",
      "0.4545\n",
      "0.4545\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.5556\n",
      "1\n",
      "1.0\n",
      "1\n",
      "0.625\n",
      "1.0\n",
      "0.5556\n",
      "1.0\n",
      "0.5556\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.625\n",
      "0.8333\n",
      "0.0\n",
      "0.3333\n",
      "1\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1\n",
      "0.3571\n",
      "0.6667\n",
      "1\n",
      "0.0\n",
      "0.0\n",
      "0.5882\n",
      "0.8333\n",
      "0.4762\n",
      "0.9375\n",
      "0.4167\n",
      "1.0\n",
      "0.8333\n",
      "1\n",
      "0.0\n",
      "1.0\n",
      "0.3125\n",
      "1\n",
      "0.3125\n",
      "0.5556\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.8824\n"
     ]
    }
   ],
   "source": [
    "# Output sentences with overcorrection and fine-grained edit types\n",
    "\n",
    "!python3 /Users/zhanyuxiao/Desktop/German_LLM_Project/Step11_evaluation_metric/ERRANTCode/pythonProject/errant/errant/commands/compare_m2.py -hyp 'outputs(SEEDA)/subset/BART.m2'  -ref extracted_target.m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82e722f2-95be-450d-981b-80192e2fa967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Span-Based Correction ============\n",
      "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
      "315\t246\t522\t0.5615\t0.3763\t0.5112\n",
      "==============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!errant_compare -hyp 'outputs(SEEDA)/subset/BART.m2'  -ref extracted_target.m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec66dc5-a5ba-44c8-9711-82ef9035bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/zhanyuxiao/myenv/lib/python3.12/site-packages (24.2)\n",
      "Collecting pip\n",
      "  Using cached pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: setuptools in /Users/zhanyuxiao/myenv/lib/python3.12/site-packages (73.0.1)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "Using cached setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 73.0.1\n",
      "    Uninstalling setuptools-73.0.1:\n",
      "      Successfully uninstalled setuptools-73.0.1\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-24.3.1 setuptools-75.8.0 wheel-0.45.1\n",
      "Obtaining file:///Users/zhanyuxiao/Desktop/German_LLM_Project/Step12_shooting_stage/human_evaluation/Self-experiment\n",
      "\u001b[31mERROR: file:///Users/zhanyuxiao/Desktop/German_LLM_Project/Step12_shooting_stage/human_evaluation/Self-experiment does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m/Users/zhanyuxiao/myenv/bin/python3: No module named spacy\n",
      "Loading resources...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/bin/errant_parallel\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/errant/commands/parallel_to_m2.py\", line 10, in main\n",
      "    annotator = errant.load(\"en\")\n",
      "                ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/errant/__init__.py\", line 16, in load\n",
      "    nlp = nlp or spacy.load(f\"{lang}_core_web_sm\", disable=[\"ner\"])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/spacy/__init__.py\", line 51, in load\n",
      "    return util.load_model(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/spacy/util.py\", line 472, in load_model\n",
      "    raise IOError(Errors.E050.format(name=name))\n",
      "OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n"
     ]
    }
   ],
   "source": [
    "!errant_parallel -orig INPUT.txt -cor 'Outputs(SEEDA)/subset/BART.txt' -out 'Outputs(SEEDA)/subset/BART.m2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c741f7-9403-4c69-abf0-be3e55fa9ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation complete for TransGEC.txt. M2 file saved to Outputs(SEEDA)/subset_m2/TransGEC.m2\n",
      "Annotation complete for TemplateGEC.txt. M2 file saved to Outputs(SEEDA)/subset_m2/TemplateGEC.m2\n",
      "Annotation complete for LM-Critic.txt. M2 file saved to Outputs(SEEDA)/subset_m2/LM-Critic.m2\n",
      "Annotation complete for GECToR-BERT.txt. M2 file saved to Outputs(SEEDA)/subset_m2/GECToR-BERT.m2\n",
      "Annotation complete for REF-M.txt. M2 file saved to Outputs(SEEDA)/subset_m2/REF-M.m2\n",
      "Annotation complete for T5.txt. M2 file saved to Outputs(SEEDA)/subset_m2/T5.m2\n",
      "Annotation complete for BERT-fuse.txt. M2 file saved to Outputs(SEEDA)/subset_m2/BERT-fuse.m2\n",
      "Annotation complete for GECToR-ens.txt. M2 file saved to Outputs(SEEDA)/subset_m2/GECToR-ens.m2\n",
      "Annotation complete for INPUT.txt. M2 file saved to Outputs(SEEDA)/subset_m2/INPUT.m2\n",
      "Annotation complete for UEDIN-MS.txt. M2 file saved to Outputs(SEEDA)/subset_m2/UEDIN-MS.m2\n",
      "Annotation complete for GPT-3.5.txt. M2 file saved to Outputs(SEEDA)/subset_m2/GPT-3.5.m2\n",
      "Annotation complete for REF-F.txt. M2 file saved to Outputs(SEEDA)/subset_m2/REF-F.m2\n",
      "Annotation complete for PIE.txt. M2 file saved to Outputs(SEEDA)/subset_m2/PIE.m2\n",
      "Annotation complete for Riken-Tohoku.txt. M2 file saved to Outputs(SEEDA)/subset_m2/Riken-Tohoku.m2\n",
      "Annotation complete for BART.txt. M2 file saved to Outputs(SEEDA)/subset_m2/BART.m2\n",
      "Annotation complete for BART.m2. M2 file saved to Outputs(SEEDA)/subset_m2/BART.m2\n",
      "All files processed.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'subprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 81\u001b[0m\n\u001b[1;32m     73\u001b[0m command \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/zhanyuxiao/Desktop/German_LLM_Project/Step11_evaluation_metric/ERRANTCode/pythonProject/errant/errant/commands/compare_m2.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-hyp\u001b[39m\u001b[38;5;124m\"\u001b[39m, m2_file_path, \n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-ref\u001b[39m\u001b[38;5;124m\"\u001b[39m, ref_m2_file\n\u001b[1;32m     78\u001b[0m ]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Execute the command\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241m.\u001b[39mrun(command)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComparison complete for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm2_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subprocess' is not defined"
     ]
    }
   ],
   "source": [
    "# For each system in the subset folder\n",
    "\n",
    "import os\n",
    "import errant\n",
    "\n",
    "# Load the Errant annotator\n",
    "annotator = errant.load('en')\n",
    "\n",
    "# Input file path for original sentences\n",
    "original_file_path = \"INPUT.txt\"\n",
    "\n",
    "# Folder containing the subset outputs\n",
    "subset_folder_path = \"Outputs(SEEDA)/subset/\"\n",
    "\n",
    "# Output folder for M2 files\n",
    "output_folder_path = \"Outputs(SEEDA)/subset_m2/\"\n",
    "\n",
    "# Make sure the output folder exists\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# Read original sentences from file\n",
    "with open(original_file_path, 'r', encoding='utf-8') as orig_file:\n",
    "    original_sentences = [line.strip() for line in orig_file.readlines()]\n",
    "\n",
    "# Iterate through each file in the 'subset' folder\n",
    "for corrected_file_name in os.listdir(subset_folder_path):\n",
    "    corrected_file_path = os.path.join(subset_folder_path, corrected_file_name)\n",
    "    \n",
    "    if os.path.isfile(corrected_file_path):  # Check if it's a file\n",
    "        output_file_path = os.path.join(output_folder_path, corrected_file_name.replace('.txt', '.m2'))\n",
    "\n",
    "        # Read corrected sentences from the current subset file\n",
    "        with open(corrected_file_path, 'r', encoding='utf-8') as cor_file:\n",
    "            corrected_sentences = [line.strip() for line in cor_file.readlines()]\n",
    "\n",
    "        # Open the output M2 file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            # Process each sentence pair\n",
    "            for orig_sentence, cor_sentence in zip(original_sentences, corrected_sentences):\n",
    "                # Parse the sentences\n",
    "                orig = annotator.parse(orig_sentence)\n",
    "                cor = annotator.parse(cor_sentence)\n",
    "\n",
    "                # Annotate the edits\n",
    "                edits = annotator.annotate(orig, cor)\n",
    "\n",
    "                # Write original sentence to the M2 file\n",
    "                output_file.write(f\"S {orig_sentence}\\n\")\n",
    "\n",
    "                # Write each edit to the M2 file\n",
    "                for e in edits:\n",
    "                    edit_data = [e.o_start, e.o_end, e.c_start, e.c_end, e.type]\n",
    "                    edit = annotator.import_edit(orig, cor, edit_data)\n",
    "                    output_file.write(edit.to_m2())\n",
    "                    output_file.write(\"\\n\")\n",
    "\n",
    "                # Separate sentence pairs with a newline\n",
    "                output_file.write(\"\\n\")\n",
    "\n",
    "        print(f\"Annotation complete for {corrected_file_name}. M2 file saved to {output_file_path}\")\n",
    "\n",
    "print(\"All files processed.\")\n",
    "\n",
    "# Reference M2 file (e.g., target file)\n",
    "ref_m2_file = \"extracted_target.m2\"\n",
    "\n",
    "# Iterate through each M2 file in the subset_m2 folder\n",
    "for m2_file_name in os.listdir(output_folder_path):\n",
    "    m2_file_path = os.path.join(output_folder_path, m2_file_name)\n",
    "    \n",
    "    if os.path.isfile(m2_file_path):  # Check if it's a file\n",
    "        # Prepare the command to compare M2 files\n",
    "        command = [\n",
    "            \"python3\", \n",
    "            \"/Users/zhanyuxiao/Desktop/German_LLM_Project/Step11_evaluation_metric/ERRANTCode/pythonProject/errant/errant/commands/compare_m2.py\", \n",
    "            \"-hyp\", m2_file_path, \n",
    "            \"-ref\", ref_m2_file\n",
    "        ]\n",
    "        \n",
    "        # Execute the command\n",
    "        subprocess.run(command)\n",
    "\n",
    "        print(f\"Comparison complete for {m2_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb13a8-71c2-4f34-83ee-f6daa1360a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (errant_env)",
   "language": "python",
   "name": "errant_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
